# ğŸ§ª RankSmart Testing Guide

## Overview

This guide explains how to test RankSmart's core functionality through our gateway testing system. Each gateway must pass before moving to the next.

## Prerequisites

### Required Environment Variables

```bash
# Required for all tests
OPENAI_API_KEY=your_openai_api_key_here

# Required for Gateway 4 (Integration Test)
FIRECRAWL_API_KEY=your_firecrawl_api_key_here

# Optional (for fallback)
GOOGLE_GEMINI_API_KEY=your_gemini_api_key_here
```

### Installation

```bash
# Install dependencies
npm install

# Or with yarn
yarn install
```

## Test Structure

### Gateway System

RankSmart uses a **sequential gateway testing system**:

1. **Gateway 1**: Firecrawl Module (Web Scraping)
2. **Gateway 2**: E-E-A-T Scorer (Content Quality)
3. **Gateway 3**: Technical SEO Checker
4. **Gateway 4**: Full Integration Test

Each gateway **must pass** before proceeding to the next. This ensures:
- âœ… Individual modules work correctly
- âœ… Dependencies are properly configured
- âœ… Integration is validated
- âœ… Production readiness

## Running Tests

### Run All Tests (Recommended)

```bash
npm test
```

This runs all gateways sequentially and stops at the first failure.

### Run Individual Gateways

```bash
# Gateway 1: Firecrawl Module
npm run test:gateway1

# Gateway 2: E-E-A-T Scorer
npm run test:gateway2

# Gateway 3: Technical SEO
npm run test:gateway3

# Gateway 4: Integration Test
npm run test:gateway4
```

### Run AI Brain Test

```bash
npm run test:brain
```

## Gateway Details

### Gateway 1: Firecrawl Module

**Purpose**: Validates web scraping functionality

**Tests**:
- âœ… URL scraping works
- âœ… Content extraction (markdown, HTML)
- âœ… Metadata extraction (title, description)
- âœ… Structural data (headings, images, links)
- âœ… Word count and reading time calculation

**Expected Output**:
```
âœ… Result has success property
âœ… Success is true
âœ… Data object exists
âœ… URL matches
âœ… Title extracted
âœ… Markdown content exists
âœ… Word count calculated
âœ… Headings extracted
âœ… Images extracted
âœ… Links extracted
âœ… Has content flag

ğŸ“Š Results: 11/11 checks passed
ğŸ‰ TEST GATEWAY 1: PASSED
```

**Common Issues**:
- âŒ Missing `FIRECRAWL_API_KEY` â†’ Set in `.env` file
- âŒ Network timeout â†’ Check internet connection
- âŒ Invalid URL â†’ Use a valid, accessible URL

---

### Gateway 2: E-E-A-T Scorer

**Purpose**: Validates content quality scoring algorithms

**Tests**:
- âœ… Experience score calculation
- âœ… Expertise score calculation
- âœ… Authoritativeness score calculation
- âœ… Trustworthiness score calculation
- âœ… Overall score aggregation
- âœ… Grade assignment
- âœ… Recommendations generation

**Expected Output**:
```
âœ… E-E-A-T result has success property
âœ… Overall score exists
âœ… Overall score is a number
âœ… Overall score is in valid range (0-100)
âœ… Grade exists
âœ… Breakdown object exists
âœ… Experience score exists
âœ… Expertise score exists
âœ… Authoritativeness score exists
âœ… Trustworthiness score exists
âœ… All breakdown scores are numbers
âœ… All breakdown scores are in valid range
âœ… Recommendations array exists

ğŸ“Š Results: 13/13 checks passed
ğŸ‰ TEST GATEWAY 2: PASSED
```

**Common Issues**:
- âŒ Scores out of range â†’ Algorithm bug (report to dev team)
- âŒ Missing breakdown â†’ Data structure issue

---

### Gateway 3: Technical SEO Checker

**Purpose**: Validates technical SEO analysis

**Tests**:
- âœ… Meta tags analysis
- âœ… Heading structure validation
- âœ… Image optimization checks
- âœ… Link quality analysis
- âœ… Content quality assessment
- âœ… Overall score calculation
- âœ… Issue prioritization (P0, P1, P2)

**Expected Output**:
```
âœ… Technical SEO result has success property
âœ… Overall score exists
âœ… Overall score is a number
âœ… Overall score is in valid range (0-100)
âœ… Grade exists
âœ… Breakdown object exists
âœ… Issues by priority exists
âœ… Total issues count exists
âœ… Critical issues count exists
âœ… All category scores are numbers
âœ… All category scores are in valid range

ğŸ“Š Results: 11/11 checks passed
ğŸ‰ TEST GATEWAY 3: PASSED
```

**Common Issues**:
- âŒ Missing categories â†’ Data extraction issue
- âŒ Invalid scores â†’ Algorithm bug

---

### Gateway 4: Full Integration Test

**Purpose**: Validates complete audit pipeline

**Tests**:
- âœ… HTTP endpoint works
- âœ… Request/response handling
- âœ… CORS headers set correctly
- âœ… All modules integrated
- âœ… Complete audit report generated
- âœ… Execution time tracked
- âœ… Error handling works

**Expected Output**:
```
âœ… Response status is 200
âœ… Response body exists
âœ… Success flag is true
âœ… Data object exists
âœ… URL in response
âœ… Scanned timestamp exists
âœ… Execution time recorded
âœ… Overall score exists
âœ… Overall grade exists
âœ… E-E-A-T data exists
âœ… E-E-A-T overall score
âœ… E-E-A-T breakdown exists
âœ… Technical SEO data exists
âœ… Technical SEO score
âœ… Page metadata exists
âœ… Stats object exists
âœ… CORS headers set

ğŸ“Š Results: 17/17 checks passed
ğŸ‰ TEST GATEWAY 4: PASSED
ğŸš€ READY FOR DEPLOYMENT!
```

**Common Issues**:
- âŒ Missing `FIRECRAWL_API_KEY` â†’ Test will be skipped
- âŒ Integration errors â†’ Check previous gateways
- âŒ Timeout â†’ Increase `FIRECRAWL_TIMEOUT` in `.env`

---

## Test Results Interpretation

### All Tests Passed âœ…

```
ğŸ‰ ALL TESTS PASSED!
âœ… All modules are working correctly
âœ… Integration is validated
âœ… Ready for deployment!
```

**Next Steps**:
1. Deploy to staging environment
2. Run production smoke tests
3. Deploy to production

---

### Tests Passed with Skips âš ï¸

```
âš ï¸  TESTS PASSED WITH SKIPS
âœ… 3 test(s) passed
â¸ï¸  1 test(s) skipped (missing API keys)
```

**Next Steps**:
1. Set missing API keys in `.env`
2. Re-run tests
3. Ensure all tests pass before deployment

---

### Tests Failed âŒ

```
âš ï¸  TESTS INCOMPLETE
âŒ 1 test(s) failed
â¸ï¸  0 test(s) skipped
â­ï¸  2 test(s) not run
```

**Next Steps**:
1. Review error messages
2. Fix the failing test
3. Re-run test suite
4. Do NOT proceed until all tests pass

---

## Debugging Failed Tests

### Enable Debug Logging

```bash
# Set debug mode in .env
DEBUG=true
LOG_LEVEL=DEBUG
```

### Check Logs

Each test outputs detailed logs:
- âœ… Success indicators
- âŒ Failure indicators
- ğŸ” Debug information
- ğŸ“Š Statistics

### Common Debugging Steps

1. **Check Environment Variables**
   ```bash
   # Verify .env file exists
   cat .env
   
   # Check specific variables
   echo $FIRECRAWL_API_KEY
   echo $OPENAI_API_KEY
   ```

2. **Test API Keys**
   ```bash
   # Test Firecrawl API
   curl -X POST https://api.firecrawl.dev/v1/scrape \
     -H "Authorization: Bearer $FIRECRAWL_API_KEY" \
     -H "Content-Type: application/json" \
     -d '{"url":"https://example.com"}'
   ```

3. **Check Network Connectivity**
   ```bash
   # Test internet connection
   ping google.com
   
   # Test API endpoints
   curl https://api.firecrawl.dev/v1/health
   ```

4. **Review Test Output**
   - Look for specific error messages
   - Check stack traces
   - Verify data structures

---

## Continuous Integration

### GitHub Actions

Tests run automatically on:
- âœ… Every push to `main`
- âœ… Every pull request
- âœ… Manual workflow dispatch

### Pre-deployment Checklist

Before deploying to production:

- [ ] All tests pass locally
- [ ] All tests pass in CI/CD
- [ ] Environment variables configured
- [ ] API keys valid and active
- [ ] No critical issues in logs
- [ ] Performance benchmarks met
- [ ] Security scan passed

---

## Performance Benchmarks

### Expected Execution Times

| Gateway | Expected Time | Max Time |
|---------|--------------|----------|
| Gateway 1 | 2-5s | 10s |
| Gateway 2 | <1s | 2s |
| Gateway 3 | <1s | 2s |
| Gateway 4 | 3-7s | 15s |

### Optimization Tips

1. **Reduce Timeout**: Lower `FIRECRAWL_TIMEOUT` for faster failures
2. **Parallel Tests**: Run independent tests in parallel (future)
3. **Cache Results**: Cache API responses for repeated tests (future)

---

## Troubleshooting

### Test Hangs/Freezes

**Symptoms**: Test runs indefinitely without output

**Solutions**:
1. Check network connectivity
2. Reduce `FIRECRAWL_TIMEOUT`
3. Kill process and restart
4. Check for infinite loops in code

---

### Intermittent Failures

**Symptoms**: Tests pass sometimes, fail other times

**Solutions**:
1. Check API rate limits
2. Verify network stability
3. Review retry logic
4. Check for race conditions

---

### All Tests Fail

**Symptoms**: Every test fails immediately

**Solutions**:
1. Verify `.env` file exists
2. Check Node.js version (>=18.0.0)
3. Reinstall dependencies: `npm install`
4. Clear cache: `npm cache clean --force`

---

## Support

### Getting Help

1. **Check Documentation**: Review this guide and other docs
2. **Search Issues**: Look for similar problems on GitHub
3. **Create Issue**: Open a new issue with:
   - Test output
   - Environment details
   - Steps to reproduce
   - Expected vs actual behavior

### Reporting Bugs

When reporting test failures, include:
- âœ… Full test output
- âœ… Environment variables (redact sensitive data)
- âœ… Node.js version: `node --version`
- âœ… npm version: `npm --version`
- âœ… Operating system
- âœ… Steps to reproduce

---

## Best Practices

### Before Committing Code

```bash
# Always run tests before committing
npm test

# If tests pass, commit
git add .
git commit -m "Your commit message"
git push
```

### Before Deploying

```bash
# Run full test suite
npm test

# Check for security vulnerabilities
npm audit

# Build for production
npm run build

# Deploy
npm run deploy
```

### Regular Maintenance

- ğŸ”„ Run tests weekly
- ğŸ”„ Update dependencies monthly
- ğŸ”„ Review test coverage quarterly
- ğŸ”„ Optimize slow tests as needed

---

## Future Enhancements

### Planned Improvements

- [ ] Add unit tests for individual functions
- [ ] Implement test coverage reporting
- [ ] Add performance regression tests
- [ ] Create visual regression tests
- [ ] Add load testing
- [ ] Implement E2E tests with Playwright
- [ ] Add mutation testing
- [ ] Create test data generators

---

## Conclusion

The gateway testing system ensures RankSmart's reliability and quality. By following this guide, you can:

âœ… Validate all core functionality
âœ… Catch bugs before deployment
âœ… Ensure production readiness
âœ… Maintain code quality

**Remember**: Never skip tests. They're your safety net! ğŸ›¡ï¸
